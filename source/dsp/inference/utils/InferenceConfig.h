#ifndef NN_INFERENCE_TEMPLATE_INFERENCECONFIG_H
#define NN_INFERENCE_TEMPLATE_INFERENCECONFIG_H

enum InferenceBackend {
    LIBTORCH,
    ONNX,
    TFLite
};

#define MODEL_INPUT_SIZE 1
#define MODEL_INPUT_SIZE_BACKEND 150 // Same as MODEL_INPUT_SIZE, but for streamable models
#define MODEL_OUTPUT_SIZE_BACKEND 1

#define MAX_INFERENCE_TIME 128
#define MODEL_LATENCY 0

#endif //NN_INFERENCE_TEMPLATE_INFERENCECONFIG_H