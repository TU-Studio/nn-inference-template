#ifndef NN_INFERENCE_TEMPLATE_INFERENCECONFIG_H
#define NN_INFERENCE_TEMPLATE_INFERENCECONFIG_H

enum InferenceBackend {
    LIBTORCH,
    ONNX,
    TFLite
};

#define MODEL_INPUT_SIZE 150
#define MAX_INFERENCE_TIME 4096
#define MODEL_LATENCY 0

#define MODEL_OUTPUT_SIZE 1

#endif //NN_INFERENCE_TEMPLATE_INFERENCECONFIG_H
