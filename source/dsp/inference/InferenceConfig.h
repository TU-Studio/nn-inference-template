#ifndef NN_INFERENCE_TEMPLATE_INFERENCECONFIG_H
#define NN_INFERENCE_TEMPLATE_INFERENCECONFIG_H

enum InferenceBackend {
    LIBTORCH,
    ONNX,
    TFLite
};

#define MODEL_TFLITE "model_0/model_0.tflite"
#define MODEL_LIBTORCH "model_0/model_0.pt"
#define MODELS_PATH_ONNX MODELS_PATH_TENSORFLOW
#define MODEL_ONNX "model_0/model_0-tflite.onnx"


#define MODEL_INPUT_SIZE 1
#define MODEL_INPUT_SIZE_BACKEND 150 // Same as MODEL_INPUT_SIZE, but for streamable models
#define MODEL_INPUT_SHAPE_ONNX {1, MODEL_INPUT_SIZE_BACKEND, 1}
#define MODEL_INPUT_SHAPE_TFLITE {1, MODEL_INPUT_SIZE_BACKEND, 1}
#define MODEL_INPUT_SHAPE_LIBTORCH {1, 1, MODEL_INPUT_SIZE_BACKEND}


#define MODEL_OUTPUT_SIZE_BACKEND 1

#define MAX_INFERENCE_TIME 1024
#define MODEL_LATENCY 0

#endif //NN_INFERENCE_TEMPLATE_INFERENCECONFIG_H